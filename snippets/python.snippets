priority -50

global !p

SINGLE_QUOTES = "'"
DOUBLE_QUOTES = '"'

def get_quoting_style(snip):
	style = snip.opt("g:ultisnips_python_quoting_style", "double")
	if style == 'single':
		return SINGLE_QUOTES
	return DOUBLE_QUOTES

endglobal

###########################################################################
#                            TEXTMATE SNIPPETS                            #
###########################################################################

snippet log_step "logging step useful for pandas dataframes"
def log_step(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        logger = logging.getLogger(inspect.stack()[1].function)

        tic = dt.datetime.now()
        result = func(*args, **kwargs)
        toc = str(dt.datetime.now() - tic)

        logger.info(f`!p snip.rv = get_quoting_style(snip)`[{func.__name__}] shape={result.shape} time={toc}`!p snip.rv = get_quoting_style(snip)`)
        return result
    return wrapper
endsnippet


snippet plot_count_null_col "plotnine script to plot the number of null values"
def plot_count_null_col(df, col, exclude_cols=None):
    exclude_cols = exclude_cols or []
    plotr = (
        df
        .set_index(col)
        .drop(columns=exclude_cols, axis=1)
        .isnull()
        .sum(axis=1)
        .groupby(col)
        .sum()
        .to_frame()
        .reset_index()
        .rename(columns={0: 'height'})
        .sort_values('height')
        .assign(height=lambda df: df.height / df.height.sum(),
                category=lambda df: pd.Categorical(df[col], categories=df[col], ordered=True))
    )
    
    return (
        p9.ggplot(plotr, p9.aes(x='category', y='height'))
        + p9.geom_col(color="black", fill="forestgreen", alpha=0.5)
        + p9.theme(axis_text_x=p9.element_text(rotation=45, hjust=1))
        + p9.ylim(0, 1)
        + p9.coord_flip()
        + p9.labs(title='Number of missing values', x='category' y='height')
        + p9.theme(figure_size=(10, 4))
    )
endsnippet

snippet ipd "import pandas as pd"
import pandas as pd
endsnippet

snippet inp "import numpy as np"
import numpy as np
endsnippet

snippet pdb "set pdb trace"
__import__('pdb').set_trace()
endsnippet

snippet ipdb "set ipdb trace"
__import__('ipdb').set_trace()
endsnippet

snippet setup "Setup for project"
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""The setup script."""


import setuptools


with open('README.md') as readme_file:
    readme = readme_file.read()


requirements = []
setup_requirements = []
test_requirements = [
	'pytest>=5.3.2'
]
extra_requirements = {}


setuptools.setup(
    name='$1',
    author='$2',
    author_email='$3',
    description='$4',
    url='TODO',
    license='Open source',
    packages=['$1'],
    version='0.1.0',
    install_requires=requirements,
    setup_requires=setup_requirements,
    test_suite='tests',
    tests_require=test_requirements,
    extras_require=extra_requirements,
)
endsnippet

snippet @fix "Pytest fixture"
@pytest.fixture
def ${1}():
	${2:pass}
endsnippet


snippet defb "bare function" b
def ${1:function}(`!p
if snip.indent:
	snip.rv = 'self' + (", " if len(t[2]) else "")`${2:arg1}):
	${0:${VISUAL:pass}}
endsnippet


snippet classb "bare class" b
class ${1:MyClass}:

	def __init__(self, ${2:arg1}):
		${0:${VISUAL:pass}}
endsnippet


snippet print_secret "Print secret in databriccks" b
print_secret = lambda scope, secret: print("\f".join([_ for _ in dbutils.secrets.get(scope, secret)]))
endsnippet

snippet setup_pyspark "Setup including devops"
import os
from glob import glob
from setuptools import find_packages, setup

BUILD_ID = os.environ.get("BUILD_BUILDID", "0")

with open("README.md") as f:
    readme = f.read()
requirements = [
    "pyspark>=3.0,<3.1",
]
setup_requirements = [
    "pytest-runner",
]
tests_requirements = [
    "pytest==5.4.1",
    "pytest-spark==0.6.0",
    "pytest-cov==2.8.1",
    "pytest-xdist==1.34.0",
    "pandas==1.0.3",
]
extras_requirements = {
    "dev": [
        "pre-commit==2.9.0",
        "flake8==3.7.9",
        "jupyter==1.0.0",
        "ipdb==0.13.3",
    ]
    + tests_requirements,
}
setup(
    name="package",
    author="Cor Zuurmond",
    author_email="<fill in>",
    version=f"0.1.{BUILD_ID}",
    description="<fill in>",
    packages=find_packages(),
    install_requires=requirements,
    setup_requires=setup_requirements,
    tests_require=tests_requirements,
    extras_require=extras_requirements,
    entry_points={"console_scripts": ["package=package.__main__:main"]},
)
endsnippet

snippet setup_cfg "Setup config"
[aliases]
test=pytest

[tool:pytest]
addopts = --ignore=package/databricks/tests --cov=package --cov-report=xml:test-coverage.xml --junitxml=test-output.xml
spark_options =
    spark.app.name: my-pytest-spark-tests
    spark.executor.instances: 1
    spark.jars.packages: io.delta:delta-core_2.12:0.7.0,com.databricks:spark-xml_2.12:0.11.0,org.apache.spark:spark-avro_2.12:3.0.1
    spark.sql.catalogImplementation: in-memory
    spark.sql.extensions: io.delta.sql.DeltaSparkSessionExtension
endsnippet
